# CoCoBench
## 在项目特定代码生成中评估大型语言模型
  
## 作者  
  
李佳，北京大学 \
李泳民，北京大学 \
赵云飞，北京大学  

## 关键词  
  
语言模型，代码生成，项目特定代码 
  
## 一、引言  

大型语言模型（LLM）最近在代码生成方面表现出了令人印象深刻的能力，提出了许多基准测试来评估独立代码生成，但实际的开发场景和独立代码生成之间存在差距。人类开发人员通常为特定项目编写程序，需要根据项目上下文编写非独立程序。非独立程序往往包含许多特定于项目的依赖项，即在项目上下文中定义的变量和函数。

![](pics/example.PNG){:height="70%" width="70%"}


分析500个开源项目中的200多万个函数 -> 80%的函数依赖于当前的项目，函数平均包含3.22个依赖项 -> 三种类型 \
 | 类内依赖关系 \
 | 文件内依赖关系 \
 | 跨文件依赖关系

 
显然，基于需求和项目上下文生成非独立代码在实际软件开发中至关重要，而且具有挑战性。迫切需要一个更高质量的基准来评估LLM在实际开发场景中的能力。

在本文中，我们提出了一个新的项目特定代码生成基准，名为CoCoBench, 包含121个项目的2768个测试样本


  
## 二、与其他常用基准的比较
 
![](pics/compare.PNG)

  
## 三、具体流程和细节 
  
## 四、实验结果与分析  
   
## 五、结论与展望  
  